# train model
model = Word2Vec(corpus2, min_count=1)
# summarize the loaded model
print("INFO: Model - \n" + str(model))

# summarize vocabulary
words = list(model.wv.vocab)
print("INFO: Words found - \n" + str(words)) 

# access vector for one word - specified by myword
myword = 'document'
print("INFO: Model of '" + myword + "' - \n" + str(model[myword]))
X = model[model.wv.vocab]

import numpy as np
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
result = pca.fit_transform(X)
from matplotlib import pyplot
# Plot the points
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model.wv.vocab)
for i, word in enumerate(words):
    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
# train model
model = Word2Vec(corpus, min_count=1)
# summarize the loaded model
print("INFO: Model - \n" + str(model))
INFO: Model - 
Word2Vec(vocab=12, size=100, alpha=0.025)
# summarize vocabulary
words = list(model.wv.vocab)
print("INFO: Words found - \n" + str(words))
INFO: Words found - 
['T', 'h', 'i', 's', ' ', 'a', 'n', 'p', 'l', 'e', 'r', 'y']
# access vector for one word - specified by myword
myword = 'a'
print("INFO: Model of '" + myword + "' - \n" + str(model[myword]))  
